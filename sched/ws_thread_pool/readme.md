# Work-Stealing ThreadPool

## Пререквизиты

- [fiber/mutex](/tasks/fiber/mutex)
- [fiber/channel](/tasks/fiber/channel)  
- [sched/intrusive](/tasks/sched/intrusive)
- [lf/ws_queue](/tasks/lf/ws_queue)

----

> _I suspect that making the scheduler use per-CPU
> queues together with some inter-CPU load balancing
> logic is probably trivial. Patches already exist, and I
> don’t feel that people can screw up the few hundred lines
> too badly._ 
> 
> – Linus Torvalds, 15 Dec 2001

[LKML](https://lkml.org/lkml/2001/12/15/32)

----

В этой задаче мы напишем [масштабируемый планировщик](exe/sched/tp/fast) для файберов, фьюч и stackless корутин.

Масштабируемость означает, что с ростом числа ядер планировщик будет выполнять пропорционально больше задач в единицу времени.

## Идеи

Из средств выразительности выберем для определенности файберы (но рассуждения ниже будут в равной степени справедливы и для фьюч, и для stackless корутин).

До этого момента в качестве планировщика файберов мы использовали простой пул потоков с разделяемой очередью задач.

Такая реализация пула хорошо подходит для длительных и независимых задач. Но подходит ли она файберам?

### Быстрые задачи и contention на очереди

Задачи файберов получаются очень короткими, что приводит к contention на мьютексе разделяемой очереди пула потоков, что ограничивает масштабируемость.

Убедитесь в этом, посмотрев на flame graph нагруженного `fiber::BufferedChannel`.

Если мы избавимся в планировщике от работы с разделяемыми структурами данных на быстром пути планирования в нагруженном сценарии, то получим физический параллелизм и масштабируемость.

### Коммуникация, локальность и кэши

Файберам свойственно передавать друг другу сообщения: явно (через каналы) или неявно (через критические секции мьютексов).

Планирование через общую очередь задач плохо подходит для таких сценариев: получатель может быть запущен на произвольном потоке или просто 
слишком поздно, когда предназначенные ему данные уже вытеснены из кэша.

Если получателя "сообщения" запускать на том же ядре, где работал отправитель, и сразу после него, то мы получим выигрыш за счет того, что получатель 
 прочтет предназначенное ему "сообщение" прямо из кэша процессора, ядру не придется [идти за данными в оперативную память](https://gist.github.com/jboner/2841832).

## Дизайн

Bird's-eye view

### Шардирование

У каждого потока-воркера будет собственная ограниченная очередь задач (будем называть эту очередь _локальной_), воркер работает в первую очередь с ней. 

Локальные очереди позволят воркерам
- Избегать координации друг с другом, что повышает пропускную способность планировщика.
- Кластеризовать коммуницирующие задачи (например, файберы) на одном ядре, что повышает эффективность кэшей.

С другой стороны, шардирование усложняет: 
1) Балансировку нагрузки, так как задачи могут распределиться и генерироваться на воркерах неравномерно. 
2) Парковку воркеров: больше нельзя атомарно проверить пустоту общей очереди и запарковать поток на кондваре.

### LIFO-планирование

Для более эффективного выполнения коммуницирующих задач планировщик будет поддерживать LIFO-планирование:

К локальной очереди на каждом воркере добавляется выделенный слот (будем называть его _LIFO-слот_), который имеет наивысший приоритет при выборе задачи и позволяет обойти локальную очередь. 

Задача, запланированная в LIFO-слот, запускается воркером уже на следующей итерации цикла планирования.

### Балансировка нагрузки

Для балансировки нагрузки используются два механизма:

- разделяемая очередь задач и 
- воровство задач (_work-stealing_).

Воркеры разделяют очередь задач неограниченной емкости (будем называть ее _глобальной_): в нее они выгружают излишки задач из локальной очереди, из нее они забирают новые задачи, когда локальная очередь опустошается.

Когда и локальная, и глобальная очереди оказываются пусты, воркеры воруют задачи прямо из локальных очередей других воркеров ([Work stealing](https://en.wikipedia.org/wiki/Work_stealing)).

## Компоненты

### Worker

Класс `Worker` представляет поток пула.

#### Локальная очередь

[`WorkStealingQueue`](exe/sched/tp/fast/queues/work_stealing_queue.hpp)

Циклический буфер фиксированного размера для локального планирования задач.

- Методы `TryPush` и `TryPop` вызывает только владеющий очередью воркер.
- Метод `Grab` вызывают
  - другие воркеры при воровстве задач,
  - владеющий очередью воркер для выгрузки задач в общую очередь при переполнении локальной.

#### LIFO-слот

Выделенный слот для планирования задач в сценариях коммуникации в обход локальной очереди.

### Shared State

Воркеры пула составляют распределенную систему, и, как это часто бывает в распределенных системах, им потребуется некоторое разделяемое состояние.

#### Глобальная очередь

[`GlobalQueue`](exe/sched/tp/fast/queues/global_queue.hpp)

Разделяемая очередь задач неограниченной емкости
- принимает задачи от `Submit` из внешних потоков,
- помогает балансировать задачи между воркерами.

#### Координатор

Компонент, координирующий воровство задач, парковку простаивающих без задач воркеров в `PickNext` и пробуждение воркеров в `Submit`.

Хранит список спящих воркеров.

----

## Алгоритм

Планировщик получает управление в двух случаях: 

1) Когда пользователь через `Submit` планирует новую задачу (например,  `fiber::Go` запускает новый файбер или `fiber::BufferedChannel<T>::Send` отправляет данные и "будит" получателя)
  
2) Когда текущая задача завершается (например, файбер останавливается в `fiber::Mutex::Lock`) и возвращает управление воркеру, который в свою очередь должен выбрать новую задачу для запуска 

### `Submit`

При планировании задачи нужно

1) поместить ее в одну из очередей (или LIFO-слот) и
2) просигнализировать другим воркерам о появлении новой задачи (если это требуется).

#### Выбор очереди

##### Внешний поток

Если задача планируется из внешнего (по отношению к пулу) потока, то она помещается в хвост общей очереди.

##### `Worker`

Если `Submit` вызывается из потока данного пула (т.е. из исполняемой задачи), то новая задача помещается в хвост локальной очереди текущего воркера.

###### Offload

Если локальная очередь переполняется, то
нужно выгрузить половину задач из нее в глобальную очередь с помощью метода `OffloadTasksToGlobalQueue`.

Минимизируйте работу в критической секции при выгрузке задач в глобальную очередь: реализуйте `GlobalQueue` как [интрузивный список](https://gitlab.com/Lipovsky/wheels/-/blob/master/wheels/intrusive/forward_list.hpp),
добавляйте в него пачку задач за O(1) с помощью метода `Append`.

#### Подсказки

Поддержите в методе `Submit` у `task::IScheduler` (и в `FiberHandle::Schedule`) дополнительный параметр [`SchedulerHint`](exe/sched/task/hint.hpp) – подсказку, с помощью которой пользователь (например, реализация файберов) может влиять на планирование задачи.

Подсказки недирективны: корректность кода, который дает подсказку планировщику, не должна зависеть от того, была ли подсказка учтена при планировании задачи.

##### `Next`

Если `Submit` вызывается из потока пула, то поместить задачу в LIFO-слот текущего воркра, в обход локальной очереди. 

Если слот уже занят другой задачей, то последняя вытесняется в хвост локальной очереди.

##### `Yield`

Задачи, которые планируются из потока пула, обслуживаются текущим воркером: попадают либо в его локальную очередь, либо в LIFO-слот.

Для `fiber::Yield` мы сделаем исключение: при перепланировании файбер отправится в хвост глобальной очереди.

#### Нотификация

После того как задача заняла свое место в структурах планировщика, нужно через координатор сообщить об этом другим воркерам (и, возможно, разбудить кого-то из них).

Если
- все воркеры пула работают либо
- есть воркеры в состоянии _searching_ (_spinning_ в планировщике Golang)

и будить никого не нужно, нотификация о появлении задачи должна обслуживаться координатором **без записи в память**, иначе мы получим точку контеншена.

### `PickNext`

Воркер перебирает следующие источники задач (в порядке убывания приоритета):

1) LIFO-слот
2) Локальная очередь
3) Глобальная очередь
4) Локальные очереди других воркеров (work stealing)

#### Слот LIFO

В первую очередь воркер забирает задачу из слота LIFO.

#### Локальная очередь

Если LIFO-слот оказался пуст, воркер забирает задачу из головы локальной очереди.

#### Grab

Если локальная очередь тоже пуста, воркер пытается забрать пачку задач из глобальной очереди с помощью метода `TryGrabTasksFromGlobalQueue` и переложить их в локальную очередь.

Как планировщик Golang перекладывает задачи из глобальной в локальную? Почему именно так?

#### Work stealing

Если LIFO-слот, локальная очередь и глобальная очередь оказались пусты, воркер ворует задачи напрямую из локальных очередей других воркеров.

Ограничивайте число одновременно ворующих воркеров: если в пуле остается мало работы, то воркеры не должны перехватывать задачи друг у друга. Изучите как устроено это ограничение в планировщиках Golang и Tokio.

##### Рандомизация

Рандомизируйте порядок обхода "жертв" с помощью [`mt19937_64`](https://en.cppreference.com/w/cpp/numeric/random).

Для инициализации вихря Мерсенна используйте [`twist::ed::std::random_device`](https://gitlab.com/Lipovsky/twist/-/blob/master/twist/ed/stdlike/random.hpp): он обеспечит детерминизм исполнения при запуске тестов в режиме симуляции.

В промышленных реализациях локальный PRNG оптимизируют:
- [`cheaprand`](https://github.com/golang/go/blob/774d5b366ce43ed7e304ea3917e353112df9daf7/src/runtime/rand.go#L182) в [планировщике горутин Golang](https://github.com/golang/go/blob/774d5b366ce43ed7e304ea3917e353112df9daf7/src/runtime/proc.go#L3578) 
- [`nextInt`](https://github.com/Kotlin/kotlinx.coroutines/blob/c1ba5af8c5d10a3d2f923f89b98f6ab1b394e24d/kotlinx-coroutines-core/jvm/src/scheduling/CoroutineScheduler.kt#L838) в [планировщике корутин Kotlin](https://github.com/Kotlin/kotlinx.coroutines/blob/c1ba5af8c5d10a3d2f923f89b98f6ab1b394e24d/kotlinx-coroutines-core/jvm/src/scheduling/CoroutineScheduler.kt#L987)
- [`FastRad`](https://github.com/tokio-rs/tokio/blob/d33fdd86a3de75500fe554d6547cf5ad43e006bf/tokio/src/util/rand.rs#L57) в [планировщике Tokio](https://github.com/tokio-rs/tokio/blob/d33fdd86a3de75500fe554d6547cf5ad43e006bf/tokio/src/runtime/scheduler/multi_thread/worker.rs#L143)

##### LIFO

Планировщик горутин в Golang ворует задачи из `runnext`, но с задержкой, планировщик в Tokio – не ворует. Чем обусловлен такой выбор?

Мы не будем поддерживать воровство задач из LIFO-слота.

#### Fairness

Каждая запланированная в пул потоков задача должна быть исполнена.

Однако если выбирать задачу для исполнения в соответствии с планом выше, то возможны сценарии, когда задача находится в планировщике, но не запускается сколь угодно долго:

1) Задача находится в глобальной очереди, при этом в локальных очередях всегда остаются задачи, так что воркеры не проверяют глобальную очередь.

2) Два файбера обмениваются сообщениями через каналы, по циклу запуская друг друга через LIFO-слот, так что воркер не проверяет локальную очередь.

Для решения первой проблемы периодически опрашивайте глобальную очередь задач **до** проверки LIFO-слота (вспомните фундаментальную константу Дмитрия Вьюкова – 61).

Для решения второй проблемы ограничивайте число последовательных запусков задач через LIFO-слот. Можно думать об этом так: задача, взятая из LIFO-слота, наследует квант времени предшествующей задачи.

#### Maintenance

(опционально)

Воркер, который не нашел задач для исполнения, может заняться [сборкой мусора](https://github.com/golang/go/blob/9effeeab27de2a8f75a1050ce879ba8db3abb406/src/runtime/proc.go#L3297): почистить тред-локальный список запланированных
на удаление объектов в менеджере памяти [hazard pointers](/tasks/lf/hazard_ptr).

#### Idle

Если в пуле не осталось доступных задач для исполнения, воркер должен парковаться на `futex` и не тратить процессорное время.

### Парковка

Сложность парковки состоит в том, чтобы **атомарно**
1) проверить, что ни в одной из очередей пула не осталось задач, и затем
2) запарковать поток на фьютексе.

Сложность пробуждения состоит в том, чтобы не будить воркеров
на каждый `Submit`.

Решение:
- Двухфазная парковка
- Каскадное пробуждение

Изучите [механизм парковки потоков](https://github.com/golang/go/blob/d2552037426fe5a190c74172562d897d921fe311/src/runtime/proc.go#L31) в планировщике Golang.

Изучите [реализацию](https://github.com/tokio-rs/tokio/blob/master/tokio/src/runtime/scheduler/multi_thread/idle.rs) этого механизма в планировщике [Tokio](https://github.com/tokio-rs/tokio).

## References

### Work Stealing

- [Scheduling Multithreaded Computations by Work Stealing](http://supertech.csail.mit.edu/papers/steal.pdf)
- https://en.wikipedia.org/wiki/Work_stealing

### Дизайн

- 🔥 [Go scheduler: Implementing language with lightweight concurrency](https://www.youtube.com/watch?v=-K11rY57K7k)
- [Making the Tokio scheduler 10x faster](https://tokio.rs/blog/2019-10-scheduler)
- [Threads at Scale](https://www.youtube.com/watch?v=PLApcas04V0)

### Промышленные реализации

- [Golang](https://github.com/golang/go/blob/master/src/runtime/proc.go)
- [Tokio (Rust)](https://github.com/tokio-rs/tokio/tree/master/tokio/src/runtime/scheduler/multi_thread)
- [Kotlin Coroutines](https://github.com/Kotlin/kotlinx.coroutines/blob/master/kotlinx-coroutines-core/jvm/src/scheduling/CoroutineScheduler.kt)
- [Cats Effect (Scala)](https://github.com/typelevel/cats-effect/blob/series/3.x/core/jvm/src/main/scala/cats/effect/unsafe/WorkStealingThreadPool.scala)


----

## Требования к реализации

### Contention

Если пул загружен задачами, то воркеры должны работать физически параллельно и не выполнять записи в общие ячейки памяти в цикле планирования и в `Submit`.

### Аллокации

Планировщик не должен аллоцировать динамическую память при планировании / выборе задач:
- Локальная очередь – фиксированный буфер указателей на задачи
- Глобальная очередь – интрузивный список задач

## Рекомендации

### Тестирование

В реализации возникает параметр, который влияет на качество рандомизированного тестирования в Twist – размер локальной очереди задач.

Адаптируйте его для тестирования:

```cpp
static constexpr size_t kLocalQueueCapacity = twist::build::kTwisted ? 7 : 256;
```

Маленький размер очереди позволит Twist исследовать более сложные конфигурации, возникающие при балансировке нагрузки.

### Профилирование

Профилируйте вашу реализацию на сценариях из [`workloads`](workloads/main.cpp) чтобы понять, на что вы расходуете время при планировании, изучайте [flame graphs](https://www.brendangregg.com/flamegraphs.html).

Добавляйте собственные сценарии, они помогут вам лучше понять поведение вашего планировщика.

В релизной сборке код будет собираться с хорошим аллокатором: https://github.com/microsoft/mimalloc

### Метрики

Собирайте [метрики](exe/sched/tp/fast/metrics.hpp):

- Сколько задач было запущено через LIFO-слот / локальную очередь / глобальную очередь
- Сколько раз воркеры воровали задачи, выгружали задачи в глобальную очередь, забирали задачи из глобальной очереди
- Сколько раз воркеры парковались из-за отсутствия задач
- и т.п.

Метрики помогут понять поведение вашей реализации в разных сценариях нагрузки.

Собирайте метрики для каждого воркера независимо, без разделяемых атомарных счетчиков.

### Конфигурация

Поддержите конфигурацию параметров алгоритма планирования. Так будет удобнее исследовать влияние этих параметров на поведение планировщика в разных сценариях нагрузки.

---

## Задание

1) Реализуйте шардированный планировщик с локальными очередями и общей очередью для балансировки нагрузки
2) Реализуйте work stealing
3) Поддержите подсказки и LIFO-планирование
   - Используйте подсказки в примитивах синхронизации
   - Убедитесь, что LIFO дает выигрыш в производительности
4) Реализуйте парковку воркеров и каскадное пробуждение
5) (опционально) Поддержите [таймеры](timer.md)
6) (опционально) Поддержите работу с сетью

## Тесты

Тесты проверяют не конкретную реализацию пула потоков, а `sched::ThreadPool`, определенный в [exe/sched/thread_pool.hpp](exe/sched/thread_pool.hpp).

Вы можете переключать этот alias между реализациями во время отладки.
